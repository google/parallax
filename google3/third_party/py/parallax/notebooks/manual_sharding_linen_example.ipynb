{
  "cells": [
    {
      "metadata": {
        "id": "10eWQPyFjRat"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment with Model Sharding Using Parallax — No Code Changes Needed\n",
        "\n",
        "This Colab demonstrates how to apply different sharding strategies to a pre-defined Flax Linen model using Parallax — without modifying the model definition or training loop.\n",
        "\n",
        "We use a simple decoder-only Transformer model to showcase Parallax's flexibility and ease of integration.\n"
      ]
    },
    {
      "metadata": {
        "id": "yX6CKpmM77DU"
      },
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from etils import ecolab\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec as P\n",
        "import optax\n",
        "# import treescope\n",
        "# treescope.basic_interactive_setup(autovisualize_arrays=True)\n",
        "\n",
        "with ecolab.adhoc('parallax_piper', reload='parallax'):\n",
        "  from parallax import manual_sharding_linen"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kagbM42tk4PU"
      },
      "cell_type": "markdown",
      "source": [
        "# Define a Decoder-Only Transformer Model\n",
        "\n",
        "Note that the model definition includes no sharding annotations — the model is written in standard Flax Linen, without any changes for parallelism or partitioning."
      ]
    },
    {
      "metadata": {
        "id": "G0skFLjPVBln"
      },
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  seq_len: int\n",
        "  embed_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    pos_emb = self.param(\n",
        "        \"pos_embedding\",\n",
        "        nn.initializers.normal(stddev=0.02),\n",
        "        (self.seq_len, self.embed_dim),\n",
        "    )\n",
        "    return x + pos_emb[None, :, :]\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  hidden_dim: int\n",
        "  out_dim: int\n",
        "  dropout_rate: float = 0.1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, deterministic: bool):\n",
        "    x = nn.Dense(self.hidden_dim)(x)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "    x = nn.Dense(self.out_dim)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "  embed_dim: int\n",
        "  num_heads: int\n",
        "  dropout_rate: float = 0.1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, x, deterministic: bool, mask: Optional[jnp.ndarray] = None\n",
        "  ):\n",
        "    # LayerNorm + Self-Attention + Residual\n",
        "    residual = x\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = nn.SelfAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        deterministic=deterministic,\n",
        "        use_bias=True,\n",
        "        broadcast_dropout=False,\n",
        "    )(x, mask=mask)\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "    x = x + residual\n",
        "\n",
        "    # LayerNorm + MLP + Residual\n",
        "    residual = x\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = MLP(\n",
        "        hidden_dim=4 * self.embed_dim,\n",
        "        out_dim=self.embed_dim,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "    )(x, deterministic)\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "    x = x + residual\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "  vocab_size: int\n",
        "  seq_len: int\n",
        "  embed_dim: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "  dropout_rate: float = 0.1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, input_ids, deterministic: bool = True):\n",
        "    # Token and Positional Embedding\n",
        "    x = nn.Embed(self.vocab_size, self.embed_dim)(input_ids)\n",
        "    x = PositionalEmbedding(seq_len=self.seq_len, embed_dim=self.embed_dim)(x)\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "\n",
        "    # Causal mask (decoder-only)\n",
        "    mask = nn.combine_masks(\n",
        "        nn.make_attention_mask(input_ids > 0, input_ids > 0, dtype=jnp.bool_),\n",
        "        nn.make_causal_mask(input_ids),\n",
        "    )\n",
        "\n",
        "    # Transformer decoder blocks\n",
        "    for _ in range(self.num_layers):\n",
        "      x = SelfAttentionBlock(\n",
        "          embed_dim=self.embed_dim,\n",
        "          num_heads=self.num_heads,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "      )(x, deterministic=deterministic, mask=mask)\n",
        "\n",
        "    # Final projection to vocab\n",
        "    logits = nn.Dense(self.vocab_size)(x)\n",
        "    return logits"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-zEI4aWCtU_9"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Define Sharding Rules in a Separate Configuration JSON\n",
        "\n",
        "Sharding strategies for inputs, parameters, and outputs are specified in a standalone JSON configuration — completely decoupled from the model code.\n"
      ]
    },
    {
      "metadata": {
        "id": "NhS09FyLpnbh"
      },
      "cell_type": "code",
      "source": [
        "sharding_config = {\n",
        "    \"mesh_axes\": (\"data\", \"model\"),\n",
        "    \"in\": P(\"data\", None),  # Batch x Seq\n",
        "    \"out\": P(\"data\", None),\n",
        "    \"parameters\": {\n",
        "        \"params\": {\n",
        "            # Input token embedding\n",
        "            \"Embed_0\": {\n",
        "                \"embedding\": P(\"data\", \"model\"),  # (vocab_size, embed_dim)\n",
        "            },\n",
        "            # Positional embedding (e.g., learned or sinusoidal)\n",
        "            \"PositionalEmbedding_0\": {\n",
        "                \"pos_embedding\": P(None, \"model\"),  # (seq_len, embed_dim)\n",
        "            },\n",
        "            # === Decoder Block 0 ===\n",
        "            \"SelfAttentionBlock_0\": {\n",
        "                \"SelfAttention_0\": {\n",
        "                    \"key\": {\n",
        "                        \"kernel\": P(\n",
        "                            None, \"model\", None\n",
        "                        ),  # 3D: (input_dim, num_heads, head_dim)\n",
        "                        \"bias\": P(\"model\", None),  # 2D: (num_heads, head_dim)\n",
        "                    },\n",
        "                    \"query\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"value\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"out\": {\n",
        "                        \"kernel\": P(\n",
        "                            \"model\", None, None\n",
        "                        ),  # Output projection: (num_heads, head_dim, output_dim)\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "                \"LayerNorm_0\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"LayerNorm_1\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"MLP_0\": {\n",
        "                    \"Dense_0\": {\n",
        "                        \"kernel\": P(\"model\", None),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                    \"Dense_1\": {\n",
        "                        \"kernel\": P(None, \"model\"),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            # === Decoder Block 1 ===\n",
        "            \"SelfAttentionBlock_1\": {\n",
        "                \"SelfAttention_0\": {\n",
        "                    \"key\": {\n",
        "                        \"kernel\": P(\n",
        "                            None, \"model\", None\n",
        "                        ),  # 3D: (input_dim, num_heads, head_dim)\n",
        "                        \"bias\": P(\"model\", None),  # 2D: (num_heads, head_dim)\n",
        "                    },\n",
        "                    \"query\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"value\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"out\": {\n",
        "                        \"kernel\": P(\n",
        "                            \"model\", None, None\n",
        "                        ),  # Output projection: (num_heads, head_dim, output_dim)\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "                \"LayerNorm_0\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"LayerNorm_1\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"MLP_0\": {\n",
        "                    \"Dense_0\": {\n",
        "                        \"kernel\": P(\"model\", None),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                    \"Dense_1\": {\n",
        "                        \"kernel\": P(None, \"model\"),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            # === Output head ===\n",
        "            \"Dense_0\": {\n",
        "                \"kernel\": P(\"model\", None),\n",
        "                \"bias\": P(None),\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8R0_P5Q8trVq"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Utilities and Setup"
      ]
    },
    {
      "metadata": {
        "id": "JM0MCC5FdV_F"
      },
      "cell_type": "code",
      "source": [
        "# Fake data generator for causal language modeling\n",
        "def generate_fake_lm_data(batch_size, seq_len, vocab_size, seed=0):\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "  input_ids = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)\n",
        "  # Targets: input shifted left, last token ignored (set to 0)\n",
        "  labels = jnp.roll(input_ids, shift=-1, axis=1)\n",
        "  labels = labels.at[:, -1].set(0)\n",
        "  return input_ids, labels"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IDBioePBpp9i"
      },
      "cell_type": "code",
      "source": [
        "def create_train_state(apply_fn, params, learning_rate):\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=tx)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nJycYX2-o0U_"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a Single Training Step"
      ]
    },
    {
      "metadata": {
        "id": "Oqu_vbTEpqcH"
      },
      "cell_type": "code",
      "source": [
        "# Single training step\n",
        "@jax.jit\n",
        "def train_step(state, batch_x, batch_y):\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(params, batch_x)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits, batch_y\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "  (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
        "      state.params\n",
        "  )\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == batch_y)\n",
        "  return state, {\"loss\": loss, \"accuracy\": accuracy}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "0UnQJXl2o-Eq"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the Training Loop (Unchanged Across Sharding Strategies)\n",
        "\n",
        "The training loop remains exactly the same, regardless of the chosen sharding strategy.\n"
      ]
    },
    {
      "metadata": {
        "id": "5_9NwXQopuBG"
      },
      "cell_type": "code",
      "source": [
        "def train(sharding_config):\n",
        "  batch_size = 64\n",
        "  seq_len = 128\n",
        "  vocab_size = 1024\n",
        "  learning_rate = 1e-3\n",
        "\n",
        "  # Generate fake LM data (inputs and targets)\n",
        "  X, y = generate_fake_lm_data(batch_size, seq_len, vocab_size)\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  model = DecoderOnlyTransformer(\n",
        "      vocab_size=vocab_size,\n",
        "      seq_len=seq_len,\n",
        "      embed_dim=512,\n",
        "      num_layers=2,\n",
        "      num_heads=8,\n",
        "  )\n",
        "\n",
        "  dummy_input = jnp.ones((2, seq_len), dtype=jnp.int32)\n",
        "  init_vars = model.init(rng, dummy_input)\n",
        "\n",
        "  # Shard model and get pjit-wrapped apply_fn\n",
        "  pjit_runner, sharded_params, mesh = manual_sharding_linen.shard_linen_model(\n",
        "      model, init_vars, sharding_config\n",
        "  )\n",
        "\n",
        "  # Create state with sharded params and pjit apply function\n",
        "  state = create_train_state(pjit_runner, sharded_params, learning_rate)\n",
        "\n",
        "  print(\"Parameter sharding: Embed_0, embedding\")\n",
        "  jax.debug.visualize_array_sharding(\n",
        "      state.params[\"params\"][\"Embed_0\"][\"embedding\"]\n",
        "  )\n",
        "\n",
        "  print(\"Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, kernel\")\n",
        "  jax.debug.visualize_array_sharding(\n",
        "      state.params[\"params\"][\"SelfAttentionBlock_1\"][\"MLP_0\"][\"Dense_1\"][\n",
        "          \"kernel\"\n",
        "      ]\n",
        "  )\n",
        "  print(\"Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, bias\")\n",
        "  jax.debug.visualize_array_sharding(\n",
        "      state.params[\"params\"][\"SelfAttentionBlock_1\"][\"MLP_0\"][\"Dense_1\"][\"bias\"]\n",
        "  )\n",
        "\n",
        "  print(\"Parameter sharding: Dense_0, MLP_0, kernel\")\n",
        "  jax.debug.visualize_array_sharding(\n",
        "      state.params[\"params\"][\"Dense_0\"][\"kernel\"]\n",
        "  )\n",
        "  print(\"Parameter sharding: Dense_0, MLP_0, bias\")\n",
        "  jax.debug.visualize_array_sharding(state.params[\"params\"][\"Dense_0\"][\"bias\"])\n",
        "\n",
        "  # Training loop inside mesh context\n",
        "  with mesh:\n",
        "    for epoch in range(10):\n",
        "      state, metrics = train_step(state, X, y)\n",
        "      print(\n",
        "          f\"Epoch {epoch}: Loss = {metrics['loss']:.4f}, Accuracy =\"\n",
        "          f\" {metrics['accuracy'] * 100:.2f}%\"\n",
        "      )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fpiMIH9muROr"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the Decoder with the Provided Sharding Rules"
      ]
    },
    {
      "metadata": {
        "executionInfo": {
          "elapsed": 1144,
          "status": "ok",
          "timestamp": 1753767555391,
          "user": {
            "displayName": "Stella Yan",
            "userId": "08807672918293624490"
          },
          "user_tz": 420
        },
        "id": "gzdag7D6p20L",
        "outputId": "a8c0e69a-5bab-4a6f-ac1b-1a8dcc7e4edb"
      },
      "cell_type": "code",
      "source": [
        "train(sharding_config)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sharded param ('params', 'Embed_0', 'embedding') with PartitionSpec('data', 'model')\n",
            "Sharded param ('params', 'PositionalEmbedding_0', 'pos_embedding') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_0', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'query', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'query', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'key', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'key', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'value', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'value', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'out', 'kernel') with PartitionSpec('model', None, None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'out', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_1', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_0', 'kernel') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_1', 'kernel') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_0', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'query', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'query', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'key', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'key', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'value', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'value', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'out', 'kernel') with PartitionSpec('model', None, None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'out', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_1', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_0', 'kernel') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_1', 'kernel') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'Dense_0', 'kernel') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'Dense_0', 'bias') with PartitionSpec(None,)\n",
            "Parameter sharding: Embed_0, embedding\n",
            "┌───────┬───────┐\n",
            "│ TPU 0 │ TPU 1 │\n",
            "├───────┼───────┤\n",
            "│ TPU 2 │ TPU 3 │\n",
            "├───────┼───────┤\n",
            "│ TPU 6 │ TPU 7 │\n",
            "├───────┼───────┤\n",
            "│ TPU 4 │ TPU 5 │\n",
            "└───────┴───────┘\n",
            "Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, kernel\n",
            "┌───────────┬───────────┐\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│TPU 0,2,4,6│TPU 1,3,5,7│\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "└───────────┴───────────┘\n",
            "Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, bias\n",
            "┌───────────┬───────────┐\n",
            "│TPU 0,2,4,6│TPU 1,3,5,7│\n",
            "└───────────┴───────────┘\n",
            "Parameter sharding: Dense_0, MLP_0, kernel\n",
            "┌────────────────────────────────────────────────┐\n",
            "│                                                │\n",
            "│                  TPU 0,2,4,6                   │\n",
            "│                                                │\n",
            "│                                                │\n",
            "├────────────────────────────────────────────────┤\n",
            "│                                                │\n",
            "│                  TPU 1,3,5,7                   │\n",
            "│                                                │\n",
            "│                                                │\n",
            "└────────────────────────────────────────────────┘\n",
            "Parameter sharding: Dense_0, MLP_0, bias\n",
            "┌───────────────────┐\n",
            "│TPU 0,1,2,3,4,5,6,7│\n",
            "└───────────────────┘\n",
            "Epoch 0: Loss = 7.5672, Accuracy = 0.11%\n",
            "Epoch 1: Loss = 7.3934, Accuracy = 0.67%\n",
            "Epoch 2: Loss = 6.4415, Accuracy = 3.27%\n",
            "Epoch 3: Loss = 5.5787, Accuracy = 7.28%\n",
            "Epoch 4: Loss = 4.8164, Accuracy = 19.26%\n",
            "Epoch 5: Loss = 4.1120, Accuracy = 35.79%\n",
            "Epoch 6: Loss = 3.3795, Accuracy = 58.64%\n",
            "Epoch 7: Loss = 2.7130, Accuracy = 73.89%\n",
            "Epoch 8: Loss = 2.0809, Accuracy = 86.32%\n",
            "Epoch 9: Loss = 1.4891, Accuracy = 94.14%\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "J3zXjIhbpgQe"
      },
      "cell_type": "markdown",
      "source": [
        "# Now Let's change the Sharding Rules"
      ]
    },
    {
      "metadata": {
        "id": "4_VucI4Apj8a"
      },
      "cell_type": "code",
      "source": [
        "sharding_config_2 = {\n",
        "    \"mesh_axes\": (\"data\", \"model\"),\n",
        "    \"in\": P(\"data\", None),  # Batch x Seq\n",
        "    \"out\": P(\"data\", None),\n",
        "    \"parameters\": {\n",
        "        \"params\": {\n",
        "            # Input token embedding\n",
        "            \"Embed_0\": {\n",
        "                \"embedding\": P(\n",
        "                    None, \"model\"\n",
        "                ),  # (vocab_size, embed_dim). # <-- Changed\n",
        "            },\n",
        "            # Positional embedding (e.g., learned or sinusoidal)\n",
        "            \"PositionalEmbedding_0\": {\n",
        "                \"pos_embedding\": P(None, \"model\"),  # (seq_len, embed_dim)\n",
        "            },\n",
        "            # === Decoder Block 0 ===\n",
        "            \"SelfAttentionBlock_0\": {\n",
        "                \"SelfAttention_0\": {\n",
        "                    \"key\": {\n",
        "                        \"kernel\": P(\n",
        "                            None, \"model\", None\n",
        "                        ),  # 3D: (input_dim, num_heads, head_dim)\n",
        "                        \"bias\": P(\"model\", None),  # 2D: (num_heads, head_dim)\n",
        "                    },\n",
        "                    \"query\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"value\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"out\": {\n",
        "                        \"kernel\": P(\n",
        "                            \"model\", None, None\n",
        "                        ),  # Output projection: (num_heads, head_dim, output_dim)\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "                \"LayerNorm_0\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"LayerNorm_1\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"MLP_0\": {\n",
        "                    \"Dense_0\": {\n",
        "                        \"kernel\": P(\"model\", None),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                    \"Dense_1\": {\n",
        "                        \"kernel\": P(None, \"model\"),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            # === Decoder Block 1 ===\n",
        "            \"SelfAttentionBlock_1\": {\n",
        "                \"SelfAttention_0\": {\n",
        "                    \"key\": {\n",
        "                        \"kernel\": P(\n",
        "                            None, \"model\", None\n",
        "                        ),  # 3D: (input_dim, num_heads, head_dim)\n",
        "                        \"bias\": P(\"model\", None),  # 2D: (num_heads, head_dim)\n",
        "                    },\n",
        "                    \"query\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"value\": {\n",
        "                        \"kernel\": P(None, \"model\", None),\n",
        "                        \"bias\": P(\"model\", None),\n",
        "                    },\n",
        "                    \"out\": {\n",
        "                        \"kernel\": P(\n",
        "                            \"model\", None, None\n",
        "                        ),  # Output projection: (num_heads, head_dim, output_dim)\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                },\n",
        "                \"LayerNorm_0\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"LayerNorm_1\": {\n",
        "                    \"scale\": P(\"model\"),\n",
        "                    \"bias\": P(\"model\"),\n",
        "                },\n",
        "                \"MLP_0\": {\n",
        "                    \"Dense_0\": {\n",
        "                        \"kernel\": P(\"model\", None),\n",
        "                        \"bias\": P(\"model\"),\n",
        "                    },\n",
        "                    \"Dense_1\": {\n",
        "                        \"kernel\": P(\"data\", None),  # <--- changed\n",
        "                        \"bias\": P(\"data\"),  # <--- changed\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            # === Output head ===\n",
        "            \"Dense_0\": {\n",
        "                \"kernel\": P(\"data\", \"model\"),  # <--- changed\n",
        "                \"bias\": P(\"data\"),  # <--- changed\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1yNJpyaxsej0"
      },
      "cell_type": "markdown",
      "source": [
        "# Train with New Sharding Rules—No Changes to Model or Training Code\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "executionInfo": {
          "elapsed": 5518,
          "status": "ok",
          "timestamp": 1753767579642,
          "user": {
            "displayName": "Stella Yan",
            "userId": "08807672918293624490"
          },
          "user_tz": 420
        },
        "id": "lDHvDe4eq3KW",
        "outputId": "05f2fc14-d79e-42b7-c6bc-61714e2f89fc"
      },
      "cell_type": "code",
      "source": [
        "train(sharding_config_2)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sharded param ('params', 'Embed_0', 'embedding') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'PositionalEmbedding_0', 'pos_embedding') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_0', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'query', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'query', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'key', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'key', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'value', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'value', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'out', 'kernel') with PartitionSpec('model', None, None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'SelfAttention_0', 'out', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_1', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'LayerNorm_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_0', 'kernel') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_1', 'kernel') with PartitionSpec(None, 'model')\n",
            "Sharded param ('params', 'SelfAttentionBlock_0', 'MLP_0', 'Dense_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_0', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'query', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'query', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'key', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'key', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'value', 'kernel') with PartitionSpec(None, 'model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'value', 'bias') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'out', 'kernel') with PartitionSpec('model', None, None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'SelfAttention_0', 'out', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_1', 'scale') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'LayerNorm_1', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_0', 'kernel') with PartitionSpec('model', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_0', 'bias') with PartitionSpec('model',)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_1', 'kernel') with PartitionSpec('data', None)\n",
            "Sharded param ('params', 'SelfAttentionBlock_1', 'MLP_0', 'Dense_1', 'bias') with PartitionSpec('data',)\n",
            "Sharded param ('params', 'Dense_0', 'kernel') with PartitionSpec('data', 'model')\n",
            "Sharded param ('params', 'Dense_0', 'bias') with PartitionSpec('data',)\n",
            "Parameter sharding: Embed_0, embedding\n",
            "┌───────────┬───────────┐\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│TPU 0,2,4,6│TPU 1,3,5,7│\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "│           │           │\n",
            "└───────────┴───────────┘\n",
            "Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, kernel\n",
            "┌───────┐\n",
            "│TPU 0,1│\n",
            "├───────┤\n",
            "│TPU 2,3│\n",
            "├───────┤\n",
            "│TPU 6,7│\n",
            "├───────┤\n",
            "│TPU 4,5│\n",
            "└───────┘\n",
            "Parameter sharding: SelfAttentionBlock_1, MLP_0, Dense_1, bias\n",
            "┌───────┬───────┬───────┬───────┐\n",
            "│TPU 0,1│TPU 2,3│TPU 6,7│TPU 4,5│\n",
            "└───────┴───────┴───────┴───────┘\n",
            "Parameter sharding: Dense_0, MLP_0, kernel\n",
            "┌───────────────────────┬───────────────────────┐\n",
            "│         TPU 0         │         TPU 1         │\n",
            "├───────────────────────┼───────────────────────┤\n",
            "│         TPU 2         │         TPU 3         │\n",
            "├───────────────────────┼───────────────────────┤\n",
            "│         TPU 6         │         TPU 7         │\n",
            "├───────────────────────┼───────────────────────┤\n",
            "│         TPU 4         │         TPU 5         │\n",
            "└───────────────────────┴───────────────────────┘\n",
            "Parameter sharding: Dense_0, MLP_0, bias\n",
            "┌───────┬───────┬───────┬───────┐\n",
            "│TPU 0,1│TPU 2,3│TPU 6,7│TPU 4,5│\n",
            "└───────┴───────┴───────┴───────┘\n",
            "Epoch 0: Loss = 7.5672, Accuracy = 0.11%\n",
            "Epoch 1: Loss = 7.3933, Accuracy = 0.66%\n",
            "Epoch 2: Loss = 6.4417, Accuracy = 3.27%\n",
            "Epoch 3: Loss = 5.5789, Accuracy = 7.28%\n",
            "Epoch 4: Loss = 4.8162, Accuracy = 19.30%\n",
            "Epoch 5: Loss = 4.1119, Accuracy = 35.84%\n",
            "Epoch 6: Loss = 3.3797, Accuracy = 58.65%\n",
            "Epoch 7: Loss = 2.7130, Accuracy = 73.91%\n",
            "Epoch 8: Loss = 2.0807, Accuracy = 86.32%\n",
            "Epoch 9: Loss = 1.4890, Accuracy = 94.19%\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/parallax/notebooks/Untitled1.ipynb?workspaceId=stellasyan:parallax_piper::citc",
          "timestamp": 1753768137336
        },
        {
          "file_id": "1hdNGq4v7qV02sH3wGypSURVKglI42dRU",
          "timestamp": 1753768074736
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
