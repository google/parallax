"""Model offloading utilities for running big models on small hardware."""

import functools
import inspect
from typing import Callable, Protocol, runtime_checkable

from flax import nnx
import jax
from jax.interpreters import pxla
import jax.sharding as shd


@runtime_checkable
class OffloadableLayer(Protocol):
  """Protocol for layers with custom offload logic."""

  def offload_state(
      self, *args, s_dev: jax.sharding.Sharding, **kwargs
  ) -> nnx.State:
    """Custom logic to prepare the layer's state for offloading."""
    ...


# Deprecated: please use create_offloaded_model.
def offload_train_step(
    model: nnx.Module,
    loss_fn,
    optimizer: nnx.Optimizer,
    labels: jax.Array,
    inputs: jax.Array,
) -> tuple[jax.Array, jax.Array]:
  """Runs one forward and backward pass of a model in chunks.

  This enables training a model that would otherwise be too large to fit into
  one device HBM. Currently only Sequential NNX models are supported.

  Args:
    model: An NNX Sequential model.
    loss_fn: The function with which to compute the loss.
    optimizer: An NNX optimizer.
    labels: A JAX array of the labels for this batch.
    inputs: A JAX array of the input batch.

  Returns:
    A tuple of the output logits and the final loss.
  """
  # 1. Forward pass on device for each chunk as its own jitted function.
  logits, saved_intermediates = offload_forward(model, inputs)

  # 2. Compute loss and initial gradient.
  def loss_fn_final(logits, labels):
    return loss_fn(logits=logits, labels=labels).mean()

  # Get initial gradient from final output.
  grad_loss_wrt_logits_fn = nnx.value_and_grad(loss_fn_final, argnums=0)
  loss, gradient = grad_loss_wrt_logits_fn(logits, labels)

  # 3. Work backwards collecting gradients for each chunk.
  final_grads_state = offload_backward(model, saved_intermediates, gradient)

  # 4. Update weights with full set of gradients.
  optimizer.update(model, final_grads_state)

  return logits, loss


# Deprecated: please use create_offloaded_model.
def offload_forward(
    model: nnx.Module,
    inputs: jax.Array,
) -> tuple[jax.Array, list[jax.Array]]:
  """Runs the forward pass of a NNX module in chunks.

  Currently only compatible with Sequential NNX modules.

  Args:
    model: An NNX Sequential model.
    inputs: A JAX array of the input batch.

  Returns:
    A tuple of the output logits, and a list of saved intermediate inputs
    for each layer.
  """

  @nnx.jit
  def forward(model_chunk, intermediate_result):
    return model_chunk(intermediate_result)

  # Each element in this array corresponds to the input of that layer; for
  # example element 0 is the inputs to layer 0. The last element of the array
  # are the final outputs (which become the inputs to the loss function).
  saved_intermediates = [inputs]
  for chunk in model.layers:
    logits = forward(chunk, saved_intermediates[-1])
    saved_intermediates.append(logits)

  return logits, saved_intermediates


# Deprecated: please use create_offloaded_model.
def offload_backward(
    model: nnx.Module,
    saved_intermediates: list[jax.Array],
    gradient: float,
) -> nnx.State:
  """Runs the backward pass of a NNX module in chunks.

  Currently only compatible with Sequential NNX modules.

  Args:
    model: An NNX Sequential model.
    saved_intermediates: A list of saved intermediate inputs from the forward
      pass generated by `offload_forward`.
    gradient: The initial scalar gradient of the loss with respect to the model
      outputs (dL/dy).

  Returns:
    A full nnx.State mapping representing a composite of the grads from all
    layers.
  """
  if len(saved_intermediates) != len(model.layers) + 1:
    raise ValueError(
        'The length of `saved_intermediates` must match the number of model '
        'layers plus one.'
    )

  layer_grads = {}
  for i, chunk in reversed(list(enumerate(model.layers))):
    # pylint: disable=cell-var-from-loop
    chunk_input = saved_intermediates[i]
    graphdef, state = nnx.split(chunk)

    # Define the forward function for *this specific chunk* making state an
    # explicit argument for jax.vjp.
    def chunk_fwd_for_vjp(current_state: nnx.State, current_input: jax.Array):
      # Merge state back temporarily to execute the layer
      layer_with_state = nnx.merge(graphdef, current_state)
      return layer_with_state(current_input)

    # Compute VJP. We want gradients w.r.t. state (parameters) and input.
    @nnx.jit
    def compute_vjp(state_primal, input_primal, cotangent):
      _, vjp_fn = jax.vjp(chunk_fwd_for_vjp, state_primal, input_primal)
      # vjp_fn calculates (cotangent * J) w.r.t primals.
      g_state, g_input = vjp_fn(cotangent)
      return g_state, g_input

    g_state, g_input = compute_vjp(state, chunk_input, gradient)
    layer_grads[i] = jax.device_put(g_state.raw_mapping, jax.devices('cpu')[0])

    # Update cotangent for the *next* iteration (previous layer).
    # gradient is now dLoss/d(chunk_input)
    gradient = g_input

  final_grads_state = {}
  final_grads_state['layers'] = layer_grads
  final_grads_state = nnx.State(final_grads_state)
  return final_grads_state


def remat_model(model: nnx.Module) -> nnx.Module:
  """Takes an NNX Module and returns one with all layers rematerialized."""
  # TODO(jeffcarp): Generalize this to work with non-Sequential models.
  new_model = nnx.clone(model)
  for i, layer in enumerate(new_model.layers):
    signature = inspect.signature(layer.__call__)
    # Assumes `input` will always be the first parameter of the Module.
    static_argnums = tuple(range(1, len(signature.parameters)))
    new_model.layers[i] = nnx.remat(
        new_model.layers[i],
        static_argnums=static_argnums,
    )
  return new_model


def offload_model(module: nnx.Module, s_dev):
  """Wraps each layer's call() to transfer state from host to device."""
  # TODO(jeffcarp): Support multi-device offloading.

  if len(list(module.iter_modules())) <= 1:
    raise ValueError('Offloading has no effect on single layers.')

  for path, node in module.iter_modules():
    # Only wrap non-root leaf nodes.
    if not path or len(list(node.iter_modules())) > 1:
      continue

    offload_method(node, '__call__', s_dev=s_dev)


def offload_method(
    module: nnx.Module,
    method_name: str,
    s_dev: jax.sharding.Sharding,
):
  """Wraps an individual method for offloading.

  This is useful in cases when a model has components that take inputs but
  don't use `__call__`. For example, to offload a Gemma model which has a
  separate `embedder` component with `encode` and `decode` methods:

  ```python
  def offload_gemma(graphdef, state, s_dev):
    model = nnx.merge(graphdef, state)
    parallax.offload_model(model, s_dev)
    parallax.offload_method(model.embedder, 'encode', s_dev)
    parallax.offload_method(model.embedder, 'decode', s_dev)
  ```

  Args:
    module: The NNX module instance containing the method to be offloaded.
    method_name: The name of the method to wrap (e.g., '__call__', 'encode').
    s_dev: The JAX sharding for the device (e.g., TPU) where the state will be
      moved for computation.
  """

  def wrap(call_fn):
    def wrapped(layer, *args, **kwargs):
      # Use custom `offload_state` method if the layer implements the protocol.
      if isinstance(layer, OffloadableLayer):
        dev_state = layer.offload_state(*args, s_dev=s_dev, **kwargs)
        nnx.update(layer, dev_state)
      else:
        state = nnx.state(layer)
        dev_state = jax.tree.map(lambda l: jax.device_put(l, s_dev), state)
        nnx.update(layer, dev_state)

      # Call original call function
      result = call_fn(layer, *args, **kwargs)
      return result

    return wrapped

  module.__class__ = type(
      module.__class__.__name__,
      (module.__class__,),
      {method_name: wrap(getattr(module.__class__, method_name))},
  )


def create_offloaded_model(
    create_model_fn: Callable[[], nnx.Module],
) -> tuple[nnx.GraphDef, nnx.State]:
  """Creates a model with weights offloaded to CPU using abstract initialization.

  This method avoids creating a full copy of the model in memory before
  offloading, allowing for the initialization of models larger than available
  device or default host memory.

  Args:
    create_model_fn: A function that creates the model.

  Returns:
    The model with weights offloaded to pinned host memory.
  """
  abstract_model = nnx.eval_shape(create_model_fn)
  abstract_state = nnx.state(abstract_model)
  graphdef = nnx.graphdef(abstract_model)
  mesh = pxla.thread_resources.env.physical_mesh

  if mesh.empty:
    raise ValueError('mesh is required to create offloaded model.')

  def get_sharding(leaf):
    spec = shd.PartitionSpec()
    if hasattr(leaf, 'sharding') and leaf.sharding is not None:
      spec = shd.PartitionSpec(*leaf.sharding)
    return shd.NamedSharding(mesh, spec, memory_kind='pinned_host')

  # Only apply shardings to variables, not metadata.
  out_shardings = jax.tree.map(
      get_sharding,
      abstract_state,
      is_leaf=lambda x: hasattr(x, 'sharding'),
  )

  @functools.partial(jax.jit, out_shardings=out_shardings)
  def create_sharded_state():
    model = create_model_fn()
    return nnx.state(model)

  return graphdef, create_sharded_state()
